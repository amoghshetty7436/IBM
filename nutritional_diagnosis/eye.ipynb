{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ9W3Mlpms5j",
        "outputId": "651f1095-4b49-4c69-b904-481aa03b8e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 383 files belonging to 5 classes.\n",
            "Class names: ['Bulging_Eyes', 'Cataracts', 'Crossed_Eyes', 'Glaucoma', 'Uveitis']\n",
            "Epoch 1/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 540ms/step - accuracy: 0.3791 - loss: 1.8378 - val_accuracy: 0.5714 - val_loss: 1.0996\n",
            "Epoch 2/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 105ms/step - accuracy: 0.6108 - loss: 1.1308 - val_accuracy: 0.5938 - val_loss: 0.9863\n",
            "Epoch 3/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 219ms/step - accuracy: 0.6261 - loss: 0.9924 - val_accuracy: 0.5781 - val_loss: 0.9122\n",
            "Epoch 4/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 337ms/step - accuracy: 0.7039 - loss: 0.7866 - val_accuracy: 0.8125 - val_loss: 0.5539\n",
            "Epoch 5/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.6813 - loss: 0.7638 - val_accuracy: 0.8906 - val_loss: 0.3910\n",
            "Epoch 6/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.7670 - loss: 0.6527 - val_accuracy: 0.8750 - val_loss: 0.4990\n",
            "Epoch 7/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8333 - loss: 0.4819 - val_accuracy: 0.9219 - val_loss: 0.3523\n",
            "Epoch 8/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.7959 - loss: 0.5116 - val_accuracy: 0.9219 - val_loss: 0.2852\n",
            "Epoch 9/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.8292 - loss: 0.4787 - val_accuracy: 0.9844 - val_loss: 0.2234\n",
            "Epoch 10/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 219ms/step - accuracy: 0.8868 - loss: 0.3824 - val_accuracy: 0.9688 - val_loss: 0.2262\n",
            "Epoch 11/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8479 - loss: 0.3552 - val_accuracy: 0.9524 - val_loss: 0.2475\n",
            "Epoch 12/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.8833 - loss: 0.3327 - val_accuracy: 0.9531 - val_loss: 0.1492\n",
            "Epoch 13/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8430 - loss: 0.3624 - val_accuracy: 0.9841 - val_loss: 0.1822\n",
            "Epoch 14/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9149 - loss: 0.2995 - val_accuracy: 0.9841 - val_loss: 0.1481\n",
            "Epoch 15/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9146 - loss: 0.2631 - val_accuracy: 1.0000 - val_loss: 0.1846\n",
            "Epoch 16/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9195 - loss: 0.2202 - val_accuracy: 0.9844 - val_loss: 0.1105\n",
            "Epoch 17/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.8830 - loss: 0.2639 - val_accuracy: 0.9841 - val_loss: 0.1121\n",
            "Epoch 18/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9257 - loss: 0.2261 - val_accuracy: 0.9844 - val_loss: 0.1136\n",
            "Epoch 19/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9664 - loss: 0.1930 - val_accuracy: 0.9844 - val_loss: 0.1056\n",
            "Epoch 20/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.9645 - loss: 0.1639 - val_accuracy: 0.9688 - val_loss: 0.1293\n",
            "Epoch 21/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9414 - loss: 0.1941 - val_accuracy: 1.0000 - val_loss: 0.0902\n",
            "Epoch 22/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9792 - loss: 0.1510 - val_accuracy: 1.0000 - val_loss: 0.0519\n",
            "Epoch 23/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9405 - loss: 0.1479 - val_accuracy: 0.9841 - val_loss: 0.0945\n",
            "Epoch 24/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - accuracy: 0.9411 - loss: 0.1522 - val_accuracy: 1.0000 - val_loss: 0.0559\n",
            "Epoch 25/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9360 - loss: 0.2078 - val_accuracy: 1.0000 - val_loss: 0.0458\n",
            "Epoch 26/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9731 - loss: 0.1357 - val_accuracy: 1.0000 - val_loss: 0.0620\n",
            "Epoch 27/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9795 - loss: 0.0992 - val_accuracy: 1.0000 - val_loss: 0.0330\n",
            "Epoch 28/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9722 - loss: 0.1256 - val_accuracy: 1.0000 - val_loss: 0.0562\n",
            "Epoch 29/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.9708 - loss: 0.1204 - val_accuracy: 1.0000 - val_loss: 0.0403\n",
            "Epoch 30/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9673 - loss: 0.1168 - val_accuracy: 1.0000 - val_loss: 0.0500\n",
            "Epoch 31/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9896 - loss: 0.0846 - val_accuracy: 1.0000 - val_loss: 0.0283\n",
            "Epoch 32/32\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9908 - loss: 0.0559 - val_accuracy: 1.0000 - val_loss: 0.0333\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 100.00%\n",
            "Model saved in SavedModel format at /content/drive/MyDrive/eye_disease_saved_model.h5\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Mount Google Drive to access your dataset if needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset paths\n",
        "dataset_path = '/content/drive/MyDrive/Eye_diseases'\n",
        "\n",
        "# Step 1: Load Datasets using image_dataset_from_directory\n",
        "batch_size = 16\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Load the full dataset\n",
        "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical',\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Retrieve class names\n",
        "class_names = full_dataset.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# Step 2: Split the dataset into training, validation, and test sets\n",
        "train_size = 0.6\n",
        "val_size = 0.2\n",
        "test_size = 0.2\n",
        "\n",
        "# Calculate sizes\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(total_size * train_size)\n",
        "val_size = int(total_size * val_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Shuffle and split the dataset\n",
        "full_dataset = full_dataset.shuffle(buffer_size=1000, seed=42)\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "remaining_dataset = full_dataset.skip(train_size)\n",
        "val_dataset = remaining_dataset.take(val_size)\n",
        "test_dataset = remaining_dataset.skip(val_size)\n",
        "\n",
        "# Step 3: Normalize the datasets\n",
        "def normalize_img(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255.0, label\n",
        "\n",
        "train_dataset = train_dataset.map(normalize_img)\n",
        "val_dataset = val_dataset.map(normalize_img)\n",
        "test_dataset = test_dataset.map(normalize_img)\n",
        "\n",
        "# Step 4: Load Pre-Trained Model (MobileNetV2)\n",
        "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation=\"relu\")(x)  # Reduced size for small dataset\n",
        "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
        "predictions = Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers initially\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Step 5: Train the Model\n",
        "epochs = 32  # Adjusted epochs for smaller dataset\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(test_dataset, verbose=1)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 7: Save the Model\n",
        "# Save the model in the SavedModel format\n",
        "saved_model_path = \"/content/drive/MyDrive/eye_disease_saved_model.h5\"\n",
        "model.save(saved_model_path)\n",
        "print(f\"Model saved in SavedModel format at {saved_model_path}\")\n"
      ]
    }
  ]
}